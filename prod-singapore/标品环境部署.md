一、云资源确认 

1. 部署nacos

前提：mysql创建好，并建好nacos的数据库用户

```
cd nacos/
vim nacos.yaml ## 修改nacos连接数据库地址
创建nacos库跟表
kubectl --kubeconfig ~/.kube/config.socialhub-prod-sg -n ns-nacos apply -f nacos.yaml
创建nacos的服务暴露
```

2. 部署sftp

```
cd sftp
echo -n "Prodsh@Socialhub!sftp2023" | base64 
vim sft-secret.yaml ## 修改密码 base64加密
vim sftp-pvc.yaml ## 不同云修改storageclass类型
kubectl  -n ns-sftp apply -f sftp-pvc.yaml
kubectl  -n ns-sftp apply -f sftp-secret.yaml
kubectl  -n ns-sftp apply -f sftp-ssh-config.yaml
kubectl  -n ns-sftp apply -f sftp-svc.yaml
kubectl  -n ns-sftp apply -f sftp-dep.yaml
## 最后根据不同云创建LB的负载均衡暴露sftp供本地连接使用
```

3. 部署es + kibana + fluentd

```
cd es
kubectl apply -f elastic-certificates.yaml
kubectl apply -f elastic-credentials.yaml
vim es-master-values.yaml
vim kibana-values.yaml
helm --kubeconfig ~/.kube/config install elasticsearch-master -f es-master-values.yaml -n ns-elastic --version 7.17.3 elastic/elasticsearch
helm --kubeconfig ~/.kube/config install kibana -f kibana-values.yaml -n ns-elastic --version 7.17.3 elastic/kibana
cd fluentd
vim fluentd-es-cm.yaml
vim fluentd.yaml
kubectl =n ns-elastic apply -f fluentd.yaml 
## 最后根据不同云创建LB的负载均衡暴露kibana供本地连接使用
```

```
helm install elasticsearch-master -f es-master-values.yaml -n ns-elastic --version 7.17.3 elastic/elasticsearch
helm --kubeconfig ~/.kube/config.socialhub-prod-yjd install elasticsearch-master -f es-master-values.yaml -n ns-elastic --version 7.17.3 elastic/elasticsearch
helm install kibana -f kibana-values.yaml --namespace mydlqcloud --version 7.7.1 elastic/kibana
```

4. 部署skywalking

前提：es已经装好

```
cd skywalking-9.5
kubectl -n ns-monitor apply -f 1-skywalking-oap-account.yaml
kubectl -n ns-monitor apply -f 2-skywalking-oap-cm-override.yaml
kubectl -n ns-monitor apply -f 2.1-skywalking-oap-init.yaml
kubectl -n ns-monitor apply -f 3-skywalking-oap.yaml
kubectl -n ns-monitor apply -f 4-skywalking-ui.yaml
## 最后根据不同云创建LB的负载均衡暴露skywalking-ui供本地连接使用
```

```
cd otel
kubectl -n ns-monitor apply -f otel-rbac.yaml
kubectl -n ns-monitor apply -f otel-deploy.yaml ## 修改自定以的指标在这
```

5. 安装grafana/ prometheus/ ksm/ blackbox / prometheusalert

```
cd prometheus
## 安装ksm
kubectl -n ns-monitor apply -f prometheus-state-metrics-rbac.yaml
kubectl -n ns-monitor apply -f prometheus-state-metrics.yaml
## 安装prometheus
kubectl -n ns-monitor apply -f prometheus-cm.yaml ## 具体配置按需定义
kubectl -n ns-monitor apply -f prometheus-rules.yaml
kubectl -n ns-monitor apply -f prometheus-rbac.yaml
kubectl -n ns-monitor apply -f prometheus-node-exporter.yaml
kubectl -n ns-monitor apply -f prometheus-statefulset.yaml
kubectl -n ns-monitor apply -f prometheus-pushgateway.yaml
## grafana
vim grafana-conf.yaml ## 修改domain为LB的公网IP地址
kubectl -n ns-monitor apply -f grafana-conf.yaml
kubectl -n ns-monitor apply -f grafana.yaml
## 安装prometheusalert
cd prometheusalert
数据库中导入prometheusalrt库和表，并建一个专门的用户用于prometheusalert服务连接
kubectl -n ns-monitor apply -f prometheusalert-dep.yaml

## 最后根据不同云创建LB的负载均衡暴露prometheus/grafana/pushgateway/alertcenter供本地连接使用

## 安装blackbox
cd blackbox
kubectl -n ns-monitor apply -f blackbox-cf.yaml
kubectl -n ns-monitor apply -f blackbox-dep.yaml
kubectl -n ns-monitor apply -f blackbox-svc.yaml

```

6. 安装apisix

```
cd apisix
kubectl -n ns-apim apply -f etcd.yaml
kubectl -n ns-apim apply -f apisix-configmap-prod.yml
kubectl -n ns-apim apply -f apisix.yaml
kubectl -n ns-apim apply -f apisix-svc.yaml
kubectl -n ns-apim apply -f apisix-dashboard.yaml
kubectl -n ns-apim apply -f apisix-dashboard-svc.yaml

最后根据不同云创建LB的负载均衡暴露apisix和apisix-dashboard供本地连接使用
```